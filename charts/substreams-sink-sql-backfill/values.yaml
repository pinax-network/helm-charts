image:
  repository: ghcr.io/pinax-network/substreams-sink-sql
  tag: develop
  pullPolicy: IfNotPresent

nameOverride: ""
fullnameOverride: ""

# Specify jobs for certain nodes
# nodeAffinity:
# - matchExpressions:
#   - key: kubernetes.io/hostname
#     operator: NotIn   # In or NotIn
#     values:
#       - domX

# Secret to pull image from private repo
imagePullSecret: ghcr-cred

# Make sure to use the service account that match with argo workflow server
serviceAccount: argo-workflows-workflow

# Time to live for workflow once it's finished
ttlStrategy:
  secondsAfterCompletion: 600
  secondsAfterFailure: 7200
  secondsAfterSuccess: 60

# Time to live for pods
# https://github.com/argoproj/argo-workflows/blob/main/examples/pod-gc-strategy.yaml
podGarbageCollection:
  strategy: OnPodCompletion
  deleteDelayDuration: 30s

retryStrategy:
  retryPolicy: "Always"
  limit: "100"
  backoff:
    duration: "1m"
    factor: "2"
    maxDuration: "6h"

# Indicate the number of parallel jobs and the resources used by a job
# This way, you can limit the total resources used with <parallelism>*<resource>
parallelism: 2
containerResources:
  requests:
    cpu: 250m
    memory: 512Mi
  limits:
    cpu: 350m
    memory: 1024Mi

# Block ranges to run. Will run as many jobs as it can based on resources available. Once the resource limit is hit, the jobs left will be put in queue
blockRanges:
  # range of blocks
  - range: [ 0, 10 ]
    # blocks per job to run for that range
    blocksPerJob: 5

# spkg file to use
manifest: myFile.spkg
# substreams endpoint
endpoint: mySubstreams:9000

# Clickhouse host name
host: myClickhouseDB
# Clickhouse cluster
cluster: myCluster
# Clickhouse database to use
database: myDatabase
# Prefix to add to cursor tables being created and deleted by the backfill jobs
# By default: backprocess-<startBlock>
tablePrefix: backprocess
# Optionally pass one or multiple headers to the Substreams endpoint (such as X-Sf-Substreams-Parallel-Jobs for example)
additionalHeaders: [ ]

# Username/Password to connect to database from an existing secret
clickhouseAuth:
  secretName: ch-auth
  keyUsername: username
  keyPassword: password
# environment variables to be use by the applications <key>: <value>
env:
  sink_sql_run_undo_buffer_size: 100
  sink_sql_run_batch_row_flush_interval: 100000
  sink_sql_run_batch_block_flush_interval: 20000
  sink_sql_run_live_block_flush_interval: 10
  sink_sql_run_on_module_hash_mistmatch: warn
  sink_sql_run_plaintext: true